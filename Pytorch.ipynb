{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b65398c-6bc1-4822-9e07-b0e8109e4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Function to download and prepare ETF data\n",
    "def prepare_etf_data(tickers, start_date, end_date):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        from curl_cffi import requests\n",
    "        session = requests.Session(impersonate=\"chrome\")\n",
    "        ticker = yf.Ticker('...', session=session)\n",
    "        # Download historical data\n",
    "        df = yf.download(ticker, start=start_date, end=end_date)\n",
    "        \n",
    "        # Calculate technical indicators\n",
    "        df['returns'] = df['Adj Close'].pct_change()\n",
    "        df['log_returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n",
    "        df['volatility_20d'] = df['returns'].rolling(20).std()\n",
    "        df['ma_50'] = df['Adj Close'].rolling(50).mean()\n",
    "        df['ma_200'] = df['Adj Close'].rolling(200).mean()\n",
    "        df['rsi_14'] = calculate_rsi(df['Adj Close'], 14)\n",
    "        df['atr_14'] = calculate_atr(df, 14)\n",
    "        # Add more indicators as needed\n",
    "        \n",
    "        data[ticker] = df.copy()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to create feature matrix\n",
    "def create_features(data, lookback=20):\n",
    "    features = []\n",
    "    dates = []\n",
    "    \n",
    "    for date in sorted(data['SPY'].index)[lookback:]:\n",
    "        feature_vector = []\n",
    "        valid_date = True\n",
    "        \n",
    "        # Ensure data is available for all tickers at this date\n",
    "        for ticker in data:\n",
    "            if date not in data[ticker].index:\n",
    "                valid_date = False\n",
    "                break\n",
    "        \n",
    "        if not valid_date:\n",
    "            continue\n",
    "            \n",
    "        # Extract features for each ticker\n",
    "        for ticker in data:\n",
    "            df = data[ticker]\n",
    "            idx = df.index.get_loc(date)\n",
    "            \n",
    "            # Current position indicators\n",
    "            feature_vector.extend([\n",
    "                df['ma_50'][idx] / df['ma_200'][idx] - 1,  # Golden cross indicator\n",
    "                df['volatility_20d'][idx],\n",
    "                df['rsi_14'][idx] / 100,  # Normalize RSI\n",
    "                df['atr_14'][idx] / df['Adj Close'][idx]  # Normalized ATR\n",
    "            ])\n",
    "            \n",
    "            # Recent performance\n",
    "            feature_vector.append(df['Adj Close'][idx] / df['Adj Close'][idx-20] - 1)\n",
    "            \n",
    "        # Add intermarket relationships\n",
    "        if 'SPY' in data and 'TLT' in data:\n",
    "            spy_return = data['SPY']['returns'][idx]\n",
    "            tlt_return = data['TLT']['returns'][idx]\n",
    "            feature_vector.append(spy_return / (tlt_return + 1e-10))  # Avoid division by zero\n",
    "        \n",
    "        # Add VIX if available\n",
    "        if 'VIX' in data:\n",
    "            feature_vector.append(data['VIX']['Adj Close'][idx])\n",
    "            feature_vector.append(data['VIX']['Adj Close'][idx] / data['VIX']['Adj Close'][idx-20])\n",
    "        \n",
    "        features.append(feature_vector)\n",
    "        dates.append(date)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    dates = np.array(dates)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, dates, scaler\n",
    "\n",
    "# Main data preparation function\n",
    "def prepare_data_for_training(tickers, start_date, end_date, lookback=20):\n",
    "    # Download and prepare data\n",
    "    data = prepare_etf_data(tickers, start_date, end_date)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X, dates, scaler = create_features(data, lookback)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    \n",
    "    return X_tensor, dates, scaler, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93f79d69-71c3-4e7a-b07b-7a8668625541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FeatureAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=10):\n",
    "        super(FeatureAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, encoding_dim)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Train the autoencoder\n",
    "def train_autoencoder(X_tensor, epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = FeatureAutoencoder(input_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, X_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, _ in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = model(data)\n",
    "            loss = criterion(decoded, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.6f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2657a720-a8b8-41b7-8eec-7fd312b485e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def identify_regimes(X_tensor, autoencoder, n_regimes=4):\n",
    "    # Use the autoencoder to get the encoded representation\n",
    "    with torch.no_grad():\n",
    "        encoded_features = autoencoder.encode(X_tensor).numpy()\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_regimes, random_state=42)\n",
    "    regime_labels = kmeans.fit_predict(encoded_features)\n",
    "    \n",
    "    # Visualize the regimes (optional)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(encoded_features[:, 0], encoded_features[:, 1], c=regime_labels, cmap='viridis')\n",
    "    plt.title('Market Regimes Visualization')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.colorbar(label='Regime')\n",
    "    plt.show()\n",
    "    \n",
    "    return regime_labels, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a63352f6-42ab-48eb-8765-682097f093b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegimeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_regimes):\n",
    "        super(RegimeClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(dim))\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, num_regimes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Train the regime classifier\n",
    "def train_regime_classifier(X_tensor, regime_labels, hidden_dims=[64, 32], num_regimes=4, \n",
    "                           epochs=100, batch_size=32, learning_rate=0.001):\n",
    "    # Convert labels to tensor\n",
    "    y_tensor = torch.tensor(regime_labels, dtype=torch.long)\n",
    "    \n",
    "    # Create model\n",
    "    input_dim = X_tensor.shape[1]\n",
    "    model = RegimeClassifier(input_dim, hidden_dims, num_regimes)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # Dataset and dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab88ab67-f633-4a7f-b7b5-18a1c4f41db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regime(model, X_tensor, softmax=True):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        if softmax:\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            return probabilities\n",
    "        return outputs\n",
    "\n",
    "def get_regime_probabilities(X_tensor, model):\n",
    "    probs = predict_regime(model, X_tensor, softmax=True)\n",
    "    return probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c497993e-d041-44c1-8db5-e090279e292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regime_allocations():\n",
    "    # Define allocation for each regime\n",
    "    # Format: {regime_id: {asset: weight}}\n",
    "    allocations = {\n",
    "        0: {'SPY': 0.6, 'QQQ': 0.2, 'TLT': 0.2, 'GLD': 0.0},  # Bullish regime\n",
    "        1: {'SPY': 0.3, 'QQQ': 0.1, 'TLT': 0.4, 'GLD': 0.2},  # Mixed regime\n",
    "        2: {'SPY': 0.1, 'QQQ': 0.0, 'TLT': 0.7, 'GLD': 0.2},  # Defensive regime\n",
    "        3: {'SPY': 0.0, 'QQQ': 0.0, 'TLT': 0.5, 'GLD': 0.5}   # Crisis regime\n",
    "    }\n",
    "    return allocations\n",
    "\n",
    "def calculate_portfolio_weights(regime_probs, allocations):\n",
    "    \"\"\"\n",
    "    Calculate portfolio weights based on regime probabilities\n",
    "    \n",
    "    Parameters:\n",
    "    - regime_probs: numpy array of shape (num_regimes,)\n",
    "    - allocations: dict mapping regime to asset allocations\n",
    "    \n",
    "    Returns:\n",
    "    - dict mapping asset to weight\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    all_assets = set()\n",
    "    for regime in allocations:\n",
    "        all_assets.update(allocations[regime].keys())\n",
    "    \n",
    "    weights = {asset: 0.0 for asset in all_assets}\n",
    "    \n",
    "    # Calculate weighted average based on regime probabilities\n",
    "    for regime in range(len(regime_probs)):\n",
    "        if regime in allocations:\n",
    "            prob = regime_probs[regime]\n",
    "            for asset, allocation in allocations[regime].items():\n",
    "                weights[asset] += prob * allocation\n",
    "    \n",
    "    # Normalize weights to ensure they sum to 1\n",
    "    total_weight = sum(weights.values())\n",
    "    if total_weight > 0:\n",
    "        weights = {asset: weight / total_weight for asset, weight in weights.items()}\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "302c04b5-092b-4bf0-a769-2b6764adcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_position_sizes(weights, account_value, max_risk_pct=0.02):\n",
    "    \"\"\"\n",
    "    Calculate position sizes based on portfolio weights and account value\n",
    "    \n",
    "    Parameters:\n",
    "    - weights: dict mapping asset to weight\n",
    "    - account_value: total portfolio value\n",
    "    - max_risk_pct: maximum risk percentage per trade\n",
    "    \n",
    "    Returns:\n",
    "    - dict mapping asset to position size\n",
    "    \"\"\"\n",
    "    position_sizes = {}\n",
    "    \n",
    "    for asset, weight in weights.items():\n",
    "        # Calculate target position value\n",
    "        target_value = account_value * weight\n",
    "        \n",
    "        # Adjust based on risk management\n",
    "        # (In a real implementation, you would calculate asset-specific risk)\n",
    "        risk_adjusted_value = min(target_value, account_value * max_risk_pct)\n",
    "        \n",
    "        position_sizes[asset] = risk_adjusted_value\n",
    "    \n",
    "    return position_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5bd3a2d0-6644-4390-b34e-68cf8a8e95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_strategy(model, scaler, tickers, start_date, end_date, account_value=100000):\n",
    "    \"\"\"\n",
    "    Run the market regime classification strategy\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained regime classifier model\n",
    "    - scaler: fitted StandardScaler\n",
    "    - tickers: list of tickers to trade\n",
    "    - start_date, end_date: date range for the strategy\n",
    "    - account_value: initial account value\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with strategy performance\n",
    "    \"\"\"\n",
    "    # Get regime allocations\n",
    "    allocations = get_regime_allocations()\n",
    "    \n",
    "    # Prepare data\n",
    "    data = prepare_etf_data(tickers, start_date, end_date)\n",
    "    X, dates, _, _ = create_features(data, lookback=20)\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    \n",
    "    # Get regime probabilities\n",
    "    regime_probs = get_regime_probabilities(X_tensor, model)\n",
    "    \n",
    "    # Initialize performance tracking\n",
    "    portfolio_values = [account_value]\n",
    "    current_weights = {ticker: 0.0 for ticker in tickers}\n",
    "    \n",
    "    # Run strategy day by day\n",
    "    for i in range(1, len(dates)):\n",
    "        # Calculate target weights based on regime probabilities\n",
    "        target_weights = calculate_portfolio_weights(regime_probs[i-1], allocations)\n",
    "        \n",
    "        # Calculate position sizes\n",
    "        position_sizes = calculate_position_sizes(target_weights, portfolio_values[-1])\n",
    "        \n",
    "        # Calculate returns\n",
    "        daily_return = 0\n",
    "        for ticker in tickers:\n",
    "            if ticker in data and dates[i] in data[ticker].index and dates[i-1] in data[ticker].index:\n",
    "                ticker_return = data[ticker]['Adj Close'][dates[i]] / data[ticker]['Adj Close'][dates[i-1]] - 1\n",
    "                daily_return += target_weights.get(ticker, 0) * ticker_return\n",
    "        \n",
    "        # Update portfolio value\n",
    "        new_value = portfolio_values[-1] * (1 + daily_return)\n",
    "        portfolio_values.append(new_value)\n",
    "        \n",
    "        # Update current weights\n",
    "        current_weights = target_weights\n",
    "    \n",
    "    # Create performance DataFrame\n",
    "    performance = pd.DataFrame({\n",
    "        'Date': dates[1:],\n",
    "        'Portfolio Value': portfolio_values[1:]\n",
    "    }).set_index('Date')\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6b96be4-ea26-4022-9b6f-97eb80a243cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(performance):\n",
    "    \"\"\"\n",
    "    Calculate strategy performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    - performance: DataFrame with 'Portfolio Value' column\n",
    "    \n",
    "    Returns:\n",
    "    - dict of performance metrics\n",
    "    \"\"\"\n",
    "    portfolio_values = performance['Portfolio Value']\n",
    "    returns = portfolio_values.pct_change().dropna()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_return = (portfolio_values.iloc[-1] / portfolio_values.iloc[0]) - 1\n",
    "    annual_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "    volatility = returns.std() * np.sqrt(252)\n",
    "    sharpe_ratio = annual_return / volatility if volatility > 0 else 0\n",
    "    \n",
    "    # Calculate drawdown\n",
    "    drawdown = 1 - portfolio_values / portfolio_values.cummax()\n",
    "    max_drawdown = drawdown.max()\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'Total Return': total_return,\n",
    "        'Annualized Return': annual_return,\n",
    "        'Annualized Volatility': volatility,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86bc6824-d97a-43b1-a7eb-397fdf444cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_test(tickers, start_date, end_date, window_size=252, step_size=63):\n",
    "    \"\"\"\n",
    "    Perform walk-forward testing of the strategy\n",
    "    \n",
    "    Parameters:\n",
    "    - tickers: list of tickers to trade\n",
    "    - start_date, end_date: date range for testing\n",
    "    - window_size: size of the training window in days\n",
    "    - step_size: days to move forward after each test\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with strategy performance\n",
    "    \"\"\"\n",
    "    # Convert dates to pandas datetime\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_results = []\n",
    "    \n",
    "    # Create date ranges for walk-forward testing\n",
    "    current_start = start\n",
    "    while current_start + pd.Timedelta(days=window_size+step_size) < end:\n",
    "        train_start = current_start\n",
    "        train_end = current_start + pd.Timedelta(days=window_size)\n",
    "        test_start = train_end\n",
    "        test_end = test_start + pd.Timedelta(days=step_size)\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_train, _, scaler, _ = prepare_data_for_training(tickers, train_start, train_end)\n",
    "        \n",
    "        # Train autoencoder\n",
    "        autoencoder = train_autoencoder(X_train)\n",
    "        \n",
    "        # Identify regimes\n",
    "        regime_labels, _ = identify_regimes(X_train, autoencoder)\n",
    "        \n",
    "        # Train regime classifier\n",
    "        model = train_regime_classifier(X_train, regime_labels)\n",
    "        \n",
    "        # Run strategy on test period\n",
    "        test_performance = run_strategy(model, scaler, tickers, test_start, test_end)\n",
    "        test_performance['Training Period'] = f\"{train_start.date()} to {train_end.date()}\"\n",
    "        \n",
    "        all_results.append(test_performance)\n",
    "        \n",
    "        # Move forward\n",
    "        current_start = current_start + pd.Timedelta(days=step_size)\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        combined_results = pd.concat(all_results)\n",
    "        return combined_results\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c89aa27f-03e2-4a95-8c1f-f87fdcab1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, autoencoder, scaler, kmeans, path):\n",
    "    \"\"\"\n",
    "    Save all model components\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained regime classifier\n",
    "    - autoencoder: trained feature autoencoder\n",
    "    - scaler: fitted StandardScaler\n",
    "    - kmeans: fitted KMeans clusterer\n",
    "    - path: directory to save models\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Save PyTorch models\n",
    "    torch.save(model.state_dict(), os.path.join(path, 'regime_classifier.pth'))\n",
    "    torch.save(autoencoder.state_dict(), os.path.join(path, 'autoencoder.pth'))\n",
    "    \n",
    "    # Save sklearn models\n",
    "    with open(os.path.join(path, 'scaler.pkl'), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    with open(os.path.join(path, 'kmeans.pkl'), 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "\n",
    "def load_model(path, input_dim, hidden_dims, num_regimes, encoding_dim=10):\n",
    "    \"\"\"\n",
    "    Load all model components\n",
    "    \n",
    "    Parameters:\n",
    "    - path: directory containing saved models\n",
    "    - input_dim: input dimension for models\n",
    "    - hidden_dims: hidden dimensions for classifier\n",
    "    - num_regimes: number of regimes\n",
    "    - encoding_dim: dimension of encoded features\n",
    "    \n",
    "    Returns:\n",
    "    - model, autoencoder, scaler, kmeans\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    # Initialize models\n",
    "    model = RegimeClassifier(input_dim, hidden_dims, num_regimes)\n",
    "    autoencoder = FeatureAutoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "    # Load PyTorch models\n",
    "    model.load_state_dict(torch.load(os.path.join(path, 'regime_classifier.pth')))\n",
    "    autoencoder.load_state_dict(torch.load(os.path.join(path, 'autoencoder.pth')))\n",
    "    \n",
    "    # Load sklearn models\n",
    "    with open(os.path.join(path, 'scaler.pkl'), 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    with open(os.path.join(path, 'kmeans.pkl'), 'rb') as f:\n",
    "        kmeans = pickle.load(f)\n",
    "    \n",
    "    return model, autoencoder, scaler, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cdcc1081-74a7-4996-bc17-36fe297f9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_trading_routine(model, scaler, tickers, account_value):\n",
    "    \"\"\"\n",
    "    Daily routine for trading based on market regime\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained regime classifier\n",
    "    - scaler: fitted StandardScaler\n",
    "    - tickers: list of tickers to trade\n",
    "    - account_value: current account value\n",
    "    \n",
    "    Returns:\n",
    "    - dict of trading actions\n",
    "    \"\"\"\n",
    "    # Get latest data\n",
    "    latest_data = {}\n",
    "    for ticker in tickers:\n",
    "        latest_data[ticker] = yf.download(ticker, period='60d')\n",
    "    \n",
    "    # Prepare features\n",
    "    X, _, _ = create_features(latest_data)\n",
    "    \n",
    "    # Use only the most recent data point\n",
    "    latest_features = X[-1].reshape(1, -1)\n",
    "    \n",
    "    # Scale features\n",
    "    scaled_features = scaler.transform(latest_features)\n",
    "    features_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "    \n",
    "    # Get regime probabilities\n",
    "    regime_probs = get_regime_probabilities(features_tensor, model)[0]\n",
    "    \n",
    "    # Get allocations for each regime\n",
    "    allocations = get_regime_allocations()\n",
    "    \n",
    "    # Calculate target weights\n",
    "    target_weights = calculate_portfolio_weights(regime_probs, allocations)\n",
    "    \n",
    "    # Calculate position sizes\n",
    "    position_sizes = calculate_position_sizes(target_weights, account_value)\n",
    "    \n",
    "    # Generate trading actions\n",
    "    actions = {ticker: position_sizes.get(ticker, 0) for ticker in tickers}\n",
    "    \n",
    "    return {\n",
    "        'regime_probabilities': {i: float(p) for i, p in enumerate(regime_probs)},\n",
    "        'target_weights': target_weights,\n",
    "        'trading_actions': actions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c89671bb-37aa-4f78-b16a-186db71aa60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install vectorbt if not already installed\n",
    "# !pip install vectorbt\n",
    "\n",
    "import vectorbt as vbt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f625ee03-35f5-4800-88df-a336064a3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vectorbt_backtest(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Prepare data and models for VectorBT backtesting\n",
    "    \n",
    "    Parameters:\n",
    "    - tickers: list of ETF tickers to include in the strategy\n",
    "    - start_date, end_date: backtest period\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with all prepared components for backtesting\n",
    "    \"\"\"\n",
    "    # Define training period (before backtest period)\n",
    "    train_start = pd.to_datetime(start_date) - timedelta(days=365)\n",
    "    train_end = pd.to_datetime(start_date) - timedelta(days=1)\n",
    "    \n",
    "    # Download data for training\n",
    "    print(\"Downloading training data...\")\n",
    "    training_data = prepare_etf_data(tickers, train_start.strftime('%Y-%m-%d'), \n",
    "                                    train_end.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Prepare features and train models\n",
    "    print(\"Preparing features and training models...\")\n",
    "    X_train, _, scaler, _ = prepare_data_for_training(tickers, \n",
    "                                                    train_start.strftime('%Y-%m-%d'), \n",
    "                                                    train_end.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    # Train autoencoder\n",
    "    print(\"Training autoencoder...\")\n",
    "    autoencoder = train_autoencoder(X_train)\n",
    "    \n",
    "    # Identify regimes\n",
    "    print(\"Identifying market regimes...\")\n",
    "    regime_labels, kmeans = identify_regimes(X_train, autoencoder)\n",
    "    \n",
    "    # Train regime classifier\n",
    "    print(\"Training regime classifier...\")\n",
    "    model = train_regime_classifier(X_train, regime_labels)\n",
    "    \n",
    "    # Download data for backtesting period\n",
    "    print(\"Downloading backtest data...\")\n",
    "    prices = {}\n",
    "    for ticker in tickers:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date)\n",
    "        prices[ticker] = df['Adj Close']\n",
    "    \n",
    "    prices_df = pd.DataFrame(prices)\n",
    "    \n",
    "    # Create regime allocations dictionary\n",
    "    regime_allocations = get_regime_allocations()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'autoencoder': autoencoder,\n",
    "        'scaler': scaler,\n",
    "        'kmeans': kmeans,\n",
    "        'prices_df': prices_df,\n",
    "        'regime_allocations': regime_allocations,\n",
    "        'tickers': tickers\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee24725d-a3f8-4fc4-93b0-1111040afa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regime_signals(backtest_data, window_size=20):\n",
    "    \"\"\"\n",
    "    Generate regime signals throughout the backtest period\n",
    "    \n",
    "    Parameters:\n",
    "    - backtest_data: Dictionary from prepare_vectorbt_backtest\n",
    "    - window_size: Lookback window for feature creation\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with regime probabilities for each date\n",
    "    \"\"\"\n",
    "    model = backtest_data['model']\n",
    "    scaler = backtest_data['scaler']\n",
    "    prices_df = backtest_data['prices_df']\n",
    "    tickers = backtest_data['tickers']\n",
    "    \n",
    "    # Create a dictionary to store all data\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        # Create dataframe with technical indicators\n",
    "        df = pd.DataFrame()\n",
    "        df['Adj Close'] = prices_df[ticker]\n",
    "        df['returns'] = df['Adj Close'].pct_change()\n",
    "        df['log_returns'] = np.log(df['Adj Close'] / df['Adj Close'].shift(1))\n",
    "        df['volatility_20d'] = df['returns'].rolling(20).std()\n",
    "        df['ma_50'] = df['Adj Close'].rolling(50).mean()\n",
    "        df['ma_200'] = df['Adj Close'].rolling(200).mean()\n",
    "        df['rsi_14'] = calculate_rsi(df['Adj Close'], 14)\n",
    "        df['atr_14'] = calculate_atr(df, 14)\n",
    "        data[ticker] = df.copy()\n",
    "    \n",
    "    # Create features matrix\n",
    "    X, dates, _ = create_features(data, lookback=window_size)\n",
    "    \n",
    "    # Scale features\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "    \n",
    "    # Get regime probabilities\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    regime_probs = probabilities.numpy()\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    regime_df = pd.DataFrame(\n",
    "        regime_probs, \n",
    "        index=dates,\n",
    "        columns=[f'Regime_{i}' for i in range(regime_probs.shape[1])]\n",
    "    )\n",
    "    \n",
    "    return regime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2c63140-27ad-4264-9ab9-c6c4473aef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vectorbt_backtest(backtest_data, regime_signals):\n",
    "    \"\"\"\n",
    "    Run vectorbt backtest using regime signals\n",
    "    \n",
    "    Parameters:\n",
    "    - backtest_data: Dictionary from prepare_vectorbt_backtest\n",
    "    - regime_signals: DataFrame with regime probabilities\n",
    "    \n",
    "    Returns:\n",
    "    - VectorBT Portfolio object with backtest results\n",
    "    \"\"\"\n",
    "    prices_df = backtest_data['prices_df']\n",
    "    regime_allocations = backtest_data['regime_allocations']\n",
    "    \n",
    "    # Initialize weights DataFrame with same index as prices\n",
    "    weights = pd.DataFrame(0, index=prices_df.index, columns=prices_df.columns)\n",
    "    \n",
    "    # For each date with regime signals, calculate portfolio weights\n",
    "    for date in regime_signals.index:\n",
    "        if date in weights.index:\n",
    "            # Get regime probabilities for this date\n",
    "            regime_probs = regime_signals.loc[date].values\n",
    "            \n",
    "            # Calculate weights for each asset\n",
    "            target_weights = calculate_portfolio_weights(regime_probs, regime_allocations)\n",
    "            \n",
    "            # Update weights DataFrame\n",
    "            for ticker, weight in target_weights.items():\n",
    "                if ticker in weights.columns:\n",
    "                    weights.loc[date, ticker] = weight\n",
    "    \n",
    "    # Forward fill weights to handle missing dates\n",
    "    weights = weights.fillna(method='ffill')\n",
    "    \n",
    "    # Run VectorBT portfolio backtest\n",
    "    portfolio = vbt.Portfolio.from_orders(\n",
    "        prices_df,\n",
    "        weights,\n",
    "        freq='1D',\n",
    "        init_cash=100000,\n",
    "        fees=0.001,  # 0.1% trading fee\n",
    "        slippage=0.001,  # 0.1% slippage\n",
    "        log=True\n",
    "    )\n",
    "    \n",
    "    return portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cef1f974-ff9e-4197-9f74-d515e48e1d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_backtest_results(portfolio, benchmark_ticker='SPY'):\n",
    "    \"\"\"\n",
    "    Analyze backtest results, including comparison to benchmark\n",
    "    \n",
    "    Parameters:\n",
    "    - portfolio: VectorBT Portfolio object\n",
    "    - benchmark_ticker: Ticker to use as benchmark\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of performance metrics\n",
    "    \"\"\"\n",
    "    # Get performance metrics\n",
    "    returns = portfolio.returns()\n",
    "    total_return = portfolio.total_return()\n",
    "    sharpe_ratio = portfolio.sharpe_ratio()\n",
    "    max_drawdown = portfolio.max_drawdown()\n",
    "    \n",
    "    # Get benchmark data\n",
    "    benchmark_data = portfolio.prices[benchmark_ticker]\n",
    "    benchmark_returns = benchmark_data.pct_change()\n",
    "    benchmark_total_return = (benchmark_data.iloc[-1] / benchmark_data.iloc[0]) - 1\n",
    "    \n",
    "    # Calculate beta to benchmark\n",
    "    cov = np.cov(returns.fillna(0), benchmark_returns.fillna(0))[0, 1]\n",
    "    var = np.var(benchmark_returns.fillna(0))\n",
    "    beta = cov / var if var != 0 else np.nan\n",
    "    \n",
    "    # Calculate alpha (annualized)\n",
    "    rf_rate = 0.03  # Assume 3% risk-free rate\n",
    "    trading_days = 252\n",
    "    annualized_return = (1 + total_return) ** (trading_days / len(returns)) - 1\n",
    "    annualized_benchmark_return = (1 + benchmark_total_return) ** (trading_days / len(benchmark_returns)) - 1\n",
    "    alpha = annualized_return - (rf_rate + beta * (annualized_benchmark_return - rf_rate))\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'Total Return': total_return,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown': max_drawdown,\n",
    "        'Beta': beta,\n",
    "        'Alpha': alpha,\n",
    "        'Benchmark Return': benchmark_total_return,\n",
    "        'Benchmark Annualized Return': annualized_benchmark_return\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f2d44dc-5701-4fb5-86b1-0911fdc53ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_backtest_results(portfolio, regime_signals):\n",
    "    \"\"\"\n",
    "    Create visualizations of backtest results including regime transitions\n",
    "    \n",
    "    Parameters:\n",
    "    - portfolio: VectorBT Portfolio object\n",
    "    - regime_signals: DataFrame with regime probabilities\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.dates as mdates\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    \n",
    "    # Create figure with multiple subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 18), gridspec_kw={'height_ratios': [2, 1, 1]})\n",
    "    \n",
    "    # 1. Portfolio value over time with regime background\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Plot portfolio value\n",
    "    portfolio.plot_value(ax=ax1)\n",
    "    \n",
    "    # Color backdrop based on dominant regime (could be enhanced with actual colors per regime)\n",
    "    dates = regime_signals.index\n",
    "    for i in range(len(dates)-1):\n",
    "        if i < len(dates) - 1:\n",
    "            # Determine dominant regime\n",
    "            probs = regime_signals.iloc[i].values\n",
    "            dominant_regime = np.argmax(probs)\n",
    "            \n",
    "            # Determine color based on regime\n",
    "            colors = ['green', 'blue', 'orange', 'red']  # Adjust colors based on regime meaning\n",
    "            color = colors[dominant_regime] if dominant_regime < len(colors) else 'gray'\n",
    "            \n",
    "            # Add colored background\n",
    "            start_date = mdates.date2num(dates[i])\n",
    "            end_date = mdates.date2num(dates[i+1])\n",
    "            ax1.axvspan(start_date, end_date, alpha=0.1, color=color)\n",
    "    \n",
    "    ax1.set_title('Portfolio Value with Market Regimes')\n",
    "    \n",
    "    # 2. Asset allocation over time\n",
    "    ax2 = axes[1]\n",
    "    portfolio.plot_assets(ax=ax2)\n",
    "    ax2.set_title('Asset Allocation Over Time')\n",
    "    \n",
    "    # 3. Regime probabilities over time\n",
    "    ax3 = axes[2]\n",
    "    \n",
    "    # Create a colormap for each regime\n",
    "    n_regimes = regime_signals.shape[1]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_regimes))\n",
    "    \n",
    "    # Stack plot of regime probabilities\n",
    "    ax3.stackplot(regime_signals.index, \n",
    "                 [regime_signals[f'Regime_{i}'] for i in range(n_regimes)],\n",
    "                 labels=[f'Regime {i}' for i in range(n_regimes)],\n",
    "                 colors=colors,\n",
    "                 alpha=0.7)\n",
    "    \n",
    "    ax3.set_title('Market Regime Probabilities')\n",
    "    ax3.legend(loc='upper left')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create additional performance visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Drawdown plot\n",
    "    portfolio.plot_drawdown(ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Portfolio Drawdown')\n",
    "    \n",
    "    # Monthly returns heatmap\n",
    "    portfolio.plot_monthly_returns(ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Monthly Returns')\n",
    "    \n",
    "    # Underwater plot\n",
    "    portfolio.plot_underwater(ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Underwater Plot')\n",
    "    \n",
    "    # Rolling Sharpe ratio\n",
    "    rolling_window = min(252, len(portfolio.returns()) // 2)  # Use 1 year window if possible\n",
    "    portfolio.rolling_sharpe(window=rolling_window).plot(ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Rolling Sharpe Ratio ({rolling_window}-day)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "febb4114-06db-4ac2-bb42-fbc49eb1157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_regime_strategy(tickers=['SPY', 'QQQ', 'TLT', 'GLD'], \n",
    "                           start_date='2018-01-01', \n",
    "                           end_date='2023-12-31'):\n",
    "    \"\"\"\n",
    "    Run complete backtest of the market regime classification strategy\n",
    "    \n",
    "    Parameters:\n",
    "    - tickers: List of ETF tickers to include\n",
    "    - start_date, end_date: Backtest period\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with backtest results\n",
    "    \"\"\"\n",
    "    print(\"Starting backtest preparation...\")\n",
    "    \n",
    "    # Prepare backtest components\n",
    "    backtest_data = prepare_vectorbt_backtest(tickers, start_date, end_date)\n",
    "    \n",
    "    print(\"Generating regime signals...\")\n",
    "    \n",
    "    # Generate regime signals\n",
    "    regime_signals = generate_regime_signals(backtest_data)\n",
    "    \n",
    "    print(\"Running vectorbt backtest...\")\n",
    "    \n",
    "    # Run backtest\n",
    "    portfolio = run_vectorbt_backtest(backtest_data, regime_signals)\n",
    "    \n",
    "    print(\"Analyzing results...\")\n",
    "    \n",
    "    # Analyze results\n",
    "    metrics = analyze_backtest_results(portfolio)\n",
    "    \n",
    "    print(\"Visualizing results...\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_backtest_results(portfolio, regime_signals)\n",
    "    \n",
    "    # Detailed statistics\n",
    "    stats = portfolio.stats()\n",
    "    \n",
    "    return {\n",
    "        'portfolio': portfolio,\n",
    "        'metrics': metrics,\n",
    "        'stats': stats,\n",
    "        'regime_signals': regime_signals\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10aecc09-355d-45b1-bbec-6f90238a6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting backtest preparation...\n",
      "Downloading training data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Ticker' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run backtest\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m backtest_results \u001b[38;5;241m=\u001b[39m \u001b[43mbacktest_regime_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtickers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSPY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQQQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTLT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGLD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVNQ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Added real estate ETF\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2019-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-12-31\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print key metrics\u001b[39;00m\n\u001b[0;32m      9\u001b[0m metrics \u001b[38;5;241m=\u001b[39m backtest_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[67], line 17\u001b[0m, in \u001b[0;36mbacktest_regime_strategy\u001b[1;34m(tickers, start_date, end_date)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting backtest preparation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Prepare backtest components\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m backtest_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_vectorbt_backtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating regime signals...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Generate regime signals\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 18\u001b[0m, in \u001b[0;36mprepare_vectorbt_backtest\u001b[1;34m(tickers, start_date, end_date)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Download data for training\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_etf_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Prepare features and train models\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparing features and training models...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 15\u001b[0m, in \u001b[0;36mprepare_etf_data\u001b[1;34m(tickers, start_date, end_date)\u001b[0m\n\u001b[0;32m     13\u001b[0m ticker \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m, session\u001b[38;5;241m=\u001b[39msession)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Download historical data\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43myf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate technical indicators\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdj Close\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpct_change()\n",
      "File \u001b[1;32mC:\\Python_Folder\\pyproj\\my_env2\\Lib\\site-packages\\yfinance\\utils.py:104\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[1;32m--> 104\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Python_Folder\\pyproj\\my_env2\\Lib\\site-packages\\yfinance\\multi.py:122\u001b[0m, in \u001b[0;36mdownload\u001b[1;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, interval, prepost, proxy, rounding, timeout, session, multi_level_index)\u001b[0m\n\u001b[0;32m    118\u001b[0m         ignore_tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# create ticker list\u001b[39;00m\n\u001b[0;32m    121\u001b[0m tickers \u001b[38;5;241m=\u001b[39m tickers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m--> 122\u001b[0m     tickers, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtickers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# accept isin as ticker\u001b[39;00m\n\u001b[0;32m    125\u001b[0m shared\u001b[38;5;241m.\u001b[39m_ISINS \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ticker' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# Run backtest\n",
    "backtest_results = backtest_regime_strategy(\n",
    "    tickers=['SPY', 'QQQ', 'TLT', 'GLD', 'VNQ'],  # Added real estate ETF\n",
    "    start_date='2019-01-01',\n",
    "    end_date='2023-12-31'\n",
    ")\n",
    "\n",
    "# Print key metrics\n",
    "metrics = backtest_results['metrics']\n",
    "print(f\"Total Return: {metrics['Total Return']:.2%}\")\n",
    "print(f\"Annualized Return: {metrics['Annualized Return']:.2%}\")\n",
    "print(f\"Sharpe Ratio: {metrics['Sharpe Ratio']:.2f}\")\n",
    "print(f\"Max Drawdown: {metrics['Max Drawdown']:.2%}\")\n",
    "print(f\"Alpha: {metrics['Alpha']:.2%}\")\n",
    "print(f\"Beta: {metrics['Beta']:.2f}\")\n",
    "print(f\"Benchmark Return: {metrics['Benchmark Return']:.2%}\")\n",
    "\n",
    "# Display detailed statistics\n",
    "display(backtest_results['stats'])\n",
    "\n",
    "# Access portfolio object for further analysis\n",
    "portfolio = backtest_results['portfolio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40169f70-d696-4a93-b705-e90bd742a869",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backtest_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m summary_df\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Run regime analysis\u001b[39;00m\n\u001b[0;32m     70\u001b[0m regime_analysis \u001b[38;5;241m=\u001b[39m analyze_regime_characteristics(\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mbacktest_results\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregime_signals\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     72\u001b[0m     backtest_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mportfolio\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprices\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Display regime analysis\u001b[39;00m\n\u001b[0;32m     76\u001b[0m display(regime_analysis\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegime\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsset\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'backtest_results' is not defined"
     ]
    }
   ],
   "source": [
    "def analyze_regime_characteristics(regime_signals, prices_df):\n",
    "    \"\"\"\n",
    "    Analyze the characteristics of each identified regime\n",
    "    \n",
    "    Parameters:\n",
    "    - regime_signals: DataFrame with regime probabilities\n",
    "    - prices_df: DataFrame with asset prices\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with regime characteristics\n",
    "    \"\"\"\n",
    "    # Create returns dataframe\n",
    "    returns_df = prices_df.pct_change().dropna()\n",
    "    \n",
    "    # Determine dominant regime for each date\n",
    "    dominant_regimes = pd.DataFrame(\n",
    "        np.argmax(regime_signals.values, axis=1),\n",
    "        index=regime_signals.index,\n",
    "        columns=['Dominant_Regime']\n",
    "    )\n",
    "    \n",
    "    # Merge with returns\n",
    "    combined_df = pd.merge(\n",
    "        dominant_regimes, \n",
    "        returns_df, \n",
    "        left_index=True, \n",
    "        right_index=True,\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Analyze characteristics by regime\n",
    "    regime_stats = {}\n",
    "    \n",
    "    for regime in combined_df['Dominant_Regime'].unique():\n",
    "        # Filter data for this regime\n",
    "        regime_data = combined_df[combined_df['Dominant_Regime'] == regime]\n",
    "        \n",
    "        # Calculate metrics for each asset\n",
    "        asset_stats = {}\n",
    "        for asset in returns_df.columns:\n",
    "            returns = regime_data[asset]\n",
    "            \n",
    "            asset_stats[asset] = {\n",
    "                'Mean Return': returns.mean(),\n",
    "                'Volatility': returns.std(),\n",
    "                'Sharpe': returns.mean() / returns.std() if returns.std() > 0 else 0,\n",
    "                'Max Drawdown': (1 - (1 + returns).cumprod() / (1 + returns).cumprod().cummax()).max(),\n",
    "                'Win Rate': (returns > 0).mean(),\n",
    "                'Observations': len(returns)\n",
    "            }\n",
    "        \n",
    "        regime_stats[f'Regime_{regime}'] = asset_stats\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_rows = []\n",
    "    \n",
    "    for regime, assets in regime_stats.items():\n",
    "        for asset, metrics in assets.items():\n",
    "            summary_rows.append({\n",
    "                'Regime': regime,\n",
    "                'Asset': asset,\n",
    "                **metrics\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Run regime analysis\n",
    "regime_analysis = analyze_regime_characteristics(\n",
    "    backtest_results['regime_signals'],\n",
    "    backtest_results['portfolio'].prices\n",
    ")\n",
    "\n",
    "# Display regime analysis\n",
    "display(regime_analysis.pivot(index='Regime', columns='Asset'))\n",
    "\n",
    "# Visualize regime-specific returns\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Regime', y='Mean Return', hue='Asset', data=regime_analysis)\n",
    "plt.title('Average Daily Returns by Regime and Asset')\n",
    "plt.xlabel('Market Regime')\n",
    "plt.ylabel('Mean Daily Return')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ea425-b006-4f7c-bfaf-e9a90ad6e9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
